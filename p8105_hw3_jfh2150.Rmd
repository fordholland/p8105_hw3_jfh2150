---
title: "HW 3"
author: "Ford Holland"
date: "10/5/2019"
output: github_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(readxl)
library(viridis)
library(p8105.datasets)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))

```


## Problem 1

The Instacart dataset contains `r nrow(instacart)` observations of `r ncol(instacart)` variables, describing the orders of `r instacart %>% count(user_id) %>% nrow()` Instacart customers. Only one order per customer is captured, and each row represents a product from a particular order.

Some variables of interest include the time logging values `order_hour_of_day` and `order_dow`, the time and day of the week that an order was placed. Together with the `product_name` field, these values allow Instacart to determine their busiest times and most demanded products, and predict when certain items may need to be stocked. The mean hour of the day for all orders is `r instacart %>% summarize(mean = mean(order_hour_of_day))`, or about 1:30 pm. Orders hours range from `r range(instacart$order_hour_of_day)[1]` to `r range(instacart$order_hour_of_day)[2]`, or midnight to 11 pm. 

The `days_since_prior_order` field, recording the days since the last order for a customer, might allow Instacart to reward their recurring customers through loyalty programs and offer incentives to bolster orders from infrequent customers. The mean and median days since prior order are `r round(mean(instacart$days_since_prior_order))` and `r median(instacart$days_since_prior_order)`, respectively.

There are 134 aisles captured, and the most items are ordered from aisles storing fruits, vegetables, and refrigerated goods, especially dairy products.

```{r}

# load instacart data
data("instacart")

# number of aisles 
instacart %>% 
  count(aisle_id) %>% 
  nrow()

# aisles with the most items ordered
instacart %>% 
  count(aisle_id, aisle) %>% 
  arrange(desc(n))

```


The plot below shows the number of products ordered for each aisle with more than 10000 items ordered. The figure demonstrates the significant difference in volume between the top few aisles and the rest.

```{r}

# process data for plot
ggp_orders = instacart %>% 
  group_by(aisle_id) %>% 
  mutate(n_items = n()) %>% 
  filter(n_items > 10000) %>% 
  select(aisle_id, aisle, n_items) %>% 
  distinct() %>% 
  arrange(desc(n_items)) %>% 
  ungroup()

# create bar chart for aisle volume
ggp_orders %>%   
  ggplot(aes(x = reorder(aisle, -n_items), y = n_items)) +
  geom_bar(stat = "identity") +
  labs(title = "Number of Items ordered by Aisle", x = "Aisle name", y = "Number of items ordered") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


The table below shows the three most popular items for the “baking ingredients”, “dog food care”, and “packaged vegetables fruits” aisles. It reveals that within the high-volume aisle of “packaged vegetables fruits,” there is a significant disparity even between the top three products, suggesting that the total product volume may be driven by relatively few items. 

```{r}

# process data for table
df_aisle_table = instacart %>% 
  group_by(aisle) %>% 
  filter(aisle %in% 
           c("baking ingredients",
             "dog food care",
             "packaged vegetables fruits")) %>% 
  select(aisle, product_name) %>% 
  count(aisle, product_name) %>% 
  top_n(3) %>% 
  arrange(desc(n)) %>% 
  ungroup()

# create table of 3 most popular products by aisle
df_aisle_table %>% 
  knitr::kable(
    col.names = c("Aisle name", "Product name", "Number ordered"),
    caption = "Three most ordered items for \"baking ingredients\", \"dog food care\", and \"packaged vegetables and fruits\" aisles"
  )

```


This table shows the mean hour of the day that Pink Lady Apples and Coffee Ice Cream are ordered for each day of the week. It is interesting that Coffee Ice Cream tends to be ordered later in the day than Pink Lady Apples, particularly on weekdays (besides friday, when Coffee Ice Cream is ordered earliest).

```{r}

# process data and create table
instacart %>% 
  filter(product_name == "Pink Lady Apples" | 
           product_name == "Coffee Ice Cream") %>% 
  group_by(order_dow, product_name) %>% 
  summarize(mean = mean(order_hour_of_day)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = order_dow, values_from = mean) %>% 
  knitr::kable(
    col.names = c("Product name",
                  "Sunday",
                  "Monday",
                  "Tuesday",
                  "Wednesday",
                  "Thursday",
                  "Friday",
                  "Saturday"),
    caption = "Mean hour of the day of product purcase",
    digits = 2)

```


## Problem 2

This problem uses the BRFSS data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package.

First, do some data cleaning:

- format the data to use appropriate variable names;
- focus on the “Overall Health” topic
- include only responses from “Excellent” to “Poor”
- organize responses as a factor taking levels ordered from “Poor” to “Excellent”

The chunk below loads and processes the BRFSS data. I clean the names using the `janitor` package and filter to only "Overall Health" observations and parse the response field as a factor. 

```{r}

data("brfss_smart2010")

brfss_smart2010 = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(
    location_abbr = locationabbr,
    location_desc = locationdesc,
    resp_id = respid
  ) %>% 
  filter(topic == "Overall Health") %>% 
  # no other response values to exclude for overall health
  mutate(response = response %>% 
           factor(levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) 

```


Using this dataset, do or answer the following (commenting on the results of each):

- In 2002, which states were observed at 7 or more locations? What about in 2010?

In 2002, Connecticut, Florida, Massachusetts, North Carolina, New Jersey, and Pennsylvania were observed at 7 or more locations. Pennsylvania was observed in 10 counties, the most of any state in the dataset. 

```{r}

brfss_smart2010 %>% 
  filter(year == 2002) %>% 
  count(location_abbr, location_desc) %>% 
  group_by(location_abbr) %>% 
  filter(n() >= 7) %>% 
  distinct(location_abbr)

```

In 2010, a total of 14 states, abbreviated below, were observed in 7 or more counties. Florida led the pack with observations from 41 counties. The number of observed counties per state increased significatly from 2002 to 2010. This increase suggests that the program was expanded in the year period and improved it's data collection capabilities.  

```{r}

brfss_smart2010 %>% 
  filter(year == 2010) %>% 
  count(location_abbr, location_desc) %>% 
  group_by(location_abbr) %>% 
  filter(n() >= 7) %>% 
  distinct(location_abbr)

```


- Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

The spaghetti plot seems to indicate a downward trend in data_value overall, with a lot of variation. The wide band of the plot begins between 30 and 20 in 2002, and ends around 27 and 17 in 2010. The overall peak data_values appear around 2002. 

There a few spikes in data_value that seem to common across states. Years 2003, 2006, and 2009 all look to have several lines forming peaks aroung the same time. These may indicate institutional changes in how BRFSS scored their survey responses or the populations that were surveyed.

```{r}

# create a variable that averages the data_value across locations within a state
brfss_smart2010 %>% 
  filter(response == "Excellent") %>% 
  group_by(location_abbr, year) %>% 
  mutate(data_value_avg = mean(data_value)) %>% 
  select(year, location_abbr, data_value_avg) %>% 
  ggplot(aes(x = year, y = data_value_avg)) +
  geom_line(aes(group = location_abbr, color = location_abbr)) +
  theme(legend.position = "none")

```

- Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State

The response distributions appear similar between 2006 and 2010 for New York. The 

```{r}

brfss_smart2010 %>% 
  filter(year %in% c(2006, 2010), location_abbr == "NY") %>% 
  ggplot(aes(x = data_value, fill = response)) +
  geom_density(alpha = .5) +
  facet_grid(~year)

```



## Problem 3

Accelerometers have become an appealing alternative to self-report techniques for studying physical activity in observational studies and clinical trials, largely because of their relative objectivity. During observation periods, the devices measure “activity counts” in a short period; one-minute intervals are common. Because accelerometers can be worn comfortably and unobtrusively, they produce around-the-clock observations.

This problem uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). The data can be downloaded here. In this spreadsheet, variables activity.* are the activity counts for each minute of a 24-hour day starting at midnight.

Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. 

{Describe the resulting dataset (e.g. what variables exist, how many observations, etc).}

The tidied accelerometer dataset comprises `r ncol(df_accel)` columns and `r nrow(df_accel)` rows of activity count observations on 1 patient over 35 days. 

The `value` feature contains the activity measurements for each minute of each 24 hour day. These measurements range from `r min(df_accel$value)` to `r max(df_accel$value)`, and have a mean and median of `r mean(df_accel$value)` and `r median(df_accel$value)`, respectively. 

In long format, the timepoint for each measurement is described jointly by the `week`, `day`, and `activity` columns. The week column has values of 1 to 5, representing the 5 weeks of the study. The day values contain the name of the day in character form for a particular observation. `activity`, numbererd `r min(df_accel$activity)` to `r max(df_accel$activity)` indicate the minute from midnight that a measurement was taken for each day. 

There is also a `day_id` that ranges from 1 to 35 and would seem to indicate the of the study, but it does not line up sequentially with the provided `day` column because the characters are sorted alphbetically, not in week sequence. 

{Since these values don't align with the actual order of the study, I use `week` and `day` groups to identify particular days.}  


```{r}

# read and tidy accelerometer data
df_accel = read_csv("Data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(cols = starts_with("activity"), 
               names_to = "activity",
               names_prefix = "activity_") %>% 
  mutate(
    activity = activity %>% as.numeric(),
    day = day %>% factor(
      levels = c("Sunday", 
                 "Monday", 
                 "Tuesday", 
                 "Wednesday",
                 "Thursday",
                 "Friday",
                 "Saturday")),
    is_weekend = case_when(
      day %in% c("Saturday", "Sunday") ~ 1,
      TRUE ~ 0
    )
  )

# check mapping of weekend indicator
df_accel %>% count(day, is_weekend)

```


Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

Activity seems lowest on Saturday, particularly during the last 2 weeks of the study. Friday and Wednesday have the highest activity averages, and Weeks 2 and 3 at the peak of the study have highest the cumulative activity. Week 4 the lowest cumulative activity. 

```{r}

df_accel %>% 
  group_by(day, week) %>% 
  mutate(
    daily_total = sum(value)
  ) %>% 
  distinct(day, week, daily_total) %>% 
  ungroup() %>% 
  arrange(day) %>% 
  pivot_wider(names_from = day, values_from = daily_total)

```


Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph

```{r}

summary(df_accel$activity)

df_accel %>% 
  #filter(day_id %in% c(1, 2)) %>% 
  ggplot(aes(x = activity, y = value)) +
  geom_line(aes(color = day)) +
  # scale_y_continuous(limits = c(-100, 7000)) +
  #theme(legend.position = "none")
  scale_x_continuous(breaks = seq(0, 1500, 180),
                     labels = c("12am", "3am", "6am", "9am", "12pm", "3pm", "6pm", "9pm", "!2am")) +
  labs(title = "Accelerometer data over 24 hour period for 35 days",
       x = "Time of day",
       y = "Activity count",
       color = "Day of the week")

```


<br>
<br>

